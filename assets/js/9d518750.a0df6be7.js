"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[864],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var o=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=o.createContext({}),p=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return o.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),h=p(n),d=r,m=h["".concat(l,".").concat(d)]||h[d]||u[d]||a;return n?o.createElement(m,i(i({ref:t},c),{},{components:n})):o.createElement(m,i({ref:t},c))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<a;p++)i[p]=n[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},5340:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>p});var o=n(7462),r=(n(7294),n(3905));const a={sidebar_label:"LibTorch"},i="LibTorch (PyTorch C++ Frontend)",s={unversionedId:"AI-engineering/LibTorch",id:"AI-engineering/LibTorch",title:"LibTorch (PyTorch C++ Frontend)",description:"LibTorch is the official C++ frontend for Pytorch. However, due to its lack of documentation, I encountered lots of confusions during its use. Some useful tips/tricks are listed here just FYI.",source:"@site/kb/AI-engineering/LibTorch.md",sourceDirName:"AI-engineering",slug:"/AI-engineering/LibTorch",permalink:"/kb/AI-engineering/LibTorch",draft:!1,editUrl:"https://github.com/yechs/website/edit/master/kb/AI-engineering/LibTorch.md",tags:[],version:"current",lastUpdatedAt:1625583093,formattedLastUpdatedAt:"Jul 6, 2021",frontMatter:{sidebar_label:"LibTorch"},sidebar:"kbSidebar",previous:{title:"Introduction \ud83d\udea7",permalink:"/kb/intro"},next:{title:"Export Model w/ Weights",permalink:"/kb/AI-engineering/Model-export"}},l={},p=[{value:"Performance: slower than Python",id:"performance-slower-than-python",level:2},{value:"Cross-Save/Load Tensors in Python",id:"cross-saveload-tensors-in-python",level:2},{value:"Save tensor in C++ and load in Python",id:"save-tensor-in-c-and-load-in-python",level:3},{value:"Save tensor in Python and load in C++",id:"save-tensor-in-python-and-load-in-c",level:3},{value:"Alternative: use pickle",id:"alternative-use-pickle",level:3},{value:"Multiple Input/Output for Inference",id:"multiple-inputoutput-for-inference",level:2}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"libtorch-pytorch-c-frontend"},"LibTorch (PyTorch C++ Frontend)"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://pytorch.org/cppdocs/frontend.html"},"LibTorch")," is the official C++ frontend for Pytorch. However, due to its lack of documentation, I encountered lots of confusions during its use. Some useful tips/tricks are listed here just FYI."),(0,r.kt)("p",null,"As of July 2021, this documentation is written based on my experience with libtorch v1.9"),(0,r.kt)("h2",{id:"performance-slower-than-python"},"Performance: slower than Python"),(0,r.kt)("p",null,"It is repeatedly reported that inference using LibTorch is much slower than that in Python. See discussions in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/issues/19106"},"#19106"),"."),(0,r.kt)("p",null,"There is also ",(0,r.kt)("a",{parentName:"p",href:"https://zhuanlan.zhihu.com/p/363319763"},"a ZhiHu article")," (in Chinese) that attempts to analyze this issue in-depth. The solution it proposed was to recompile libtorch by linking to libraries used by pytorch."),(0,r.kt)("h2",{id:"cross-saveload-tensors-in-python"},"Cross-Save/Load Tensors in Python"),(0,r.kt)("p",null,"This section documents how to save tensors in C++ and load them into Python, and vice versa. It is often done for more friendly debugging experience offered by the Python frontend."),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Note that LibTorch ",(0,r.kt)("inlineCode",{parentName:"p"},"torch::save()")," function (",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/blob/v1.9.0/torch/csrc/api/include/torch/serialize.h#L11-L45"},"source"),") saves the tensors in a wrapped TorchScript (JIT) module, unlike ",(0,r.kt)("inlineCode",{parentName:"p"},"torch.save()")," (",(0,r.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/1.9.0/generated/torch.save.html"},"docs"),") in Python.")),(0,r.kt)("h3",{id:"save-tensor-in-c-and-load-in-python"},"Save tensor in C++ and load in Python"),(0,r.kt)("p",null,"In C++, call ",(0,r.kt)("inlineCode",{parentName:"p"},"torch::save()")," to save."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'#include <torch/torch.h>\n\n// save one tensor\ntorch::save(tensor, "tensor.pt");\n\n// save multiple tensors\ntorch::save({tensora, tensorb, tensorc}, "tensors.pt");\n')),(0,r.kt)("p",null,"In Python, use ",(0,r.kt)("inlineCode",{parentName:"p"},"torch.jit.load()")," to load."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import torch\n\n# Load one tensor\ntensor_model = torch.jit.load("tensor.pt")\ntensor = list(tensor_model.parameters())[0]\n\n# Load multiple tensors\ntensors_model = torch.jit.load("tensors.pt")\ntensora = list(tensors_model.parameters())[0]\ntensorb = list(tensors_model.parameters())[1]\ntensorc = list(tensors_model.parameters())[2]\n')),(0,r.kt)("h3",{id:"save-tensor-in-python-and-load-in-c"},"Save tensor in Python and load in C++"),(0,r.kt)("p",null,"The following codes are adapted from ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/issues/20356#issuecomment-545572400"},"pytorch/pytorch#20356 (comment)")," and updated for the v1.8+ API (",(0,r.kt)("inlineCode",{parentName:"p"},"get_attribute")," => ",(0,r.kt)("inlineCode",{parentName:"p"},"attr"),")."),(0,r.kt)("p",null,"Save tensors in Python: to do so, you have to create a model and include all tensors into this TorchScript module."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import torch\n\nclass Container(torch.nn.Module):\n    def __init__(self, my_values):\n        super().__init__()\n        for key in my_values:\n            setattr(self, key, my_values[key])\n\nmy_values = {\n    'a': torch.ones(2, 2),\n    'b': torch.ones(2, 2) + 10,\n    'c': 'hello',\n    'd': 6\n}\n\n# Save arbitrary values supported by TorchScript\n# https://pytorch.org/docs/master/jit.html#supported-type\ncontainer = torch.jit.script(Container(my_values))\ncontainer.save(\"container.pt\")\n")),(0,r.kt)("p",null,"Load tensors in C++"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'#include <torch/script.h>\n\ntorch::jit::script::Module container = torch::jit::load("container.pt");\n\ntorch::Tensor a = container.attr("a").toTensor();\ntorch::Tensor b = container.attr("b").toTensor();\nstd::string c = container.attr("c").toStringRef();\n\nint64_t d = container.attr("d").toInt();\n')),(0,r.kt)("h3",{id:"alternative-use-pickle"},"Alternative: use pickle"),(0,r.kt)("p",null,"An alternative is to use ",(0,r.kt)("inlineCode",{parentName:"p"},"pickle_save")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"pickle_load")," (",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/blob/v1.9.0/torch/csrc/api/include/torch/serialize.h#L76-L77"},"source"),"). See ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/issues/20356#issuecomment-782341150"},"this comment in pytorch/pytorch#20356")," for usage."),(0,r.kt)("h2",{id:"multiple-inputoutput-for-inference"},"Multiple Input/Output for Inference"),(0,r.kt)("p",null,"Suppose we have loaded a model named ",(0,r.kt)("inlineCode",{parentName:"p"},"module")," and want to use it for inference. However, the model requires multiple inputs/outputs."),(0,r.kt)("p",null,"The codes are adapted from ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/issues/18337"},"pytorch/pytorch#18337"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},"std::vector<torch::jit::IValue> inputs;\ninputs.push_back(tensora);\ninputs.push_back(tensorb);\n\nauto outputs = module->forward(inputs).toTuple();\n\ntorch::Tensor out1 = outputs->elements()[0].toTensor();\ntorch::Tensor out2 = outputs->elements()[1].toTensor();\n")),(0,r.kt)("admonition",{type:"info"},(0,r.kt)("p",{parentName:"admonition"},"Note: if you only have one output, you can directly call ",(0,r.kt)("inlineCode",{parentName:"p"},"toTensor()")," on the output of ",(0,r.kt)("inlineCode",{parentName:"p"},"forward()"),".")))}u.isMDXComponent=!0}}]);